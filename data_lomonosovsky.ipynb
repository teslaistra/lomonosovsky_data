{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import  os\n",
    "import json\n",
    "import re\n",
    "import urllib as urllib2\n",
    "from urllib import request\n",
    "import requests\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.reformagkh.ru'\n",
    "EXTENDED_BASE = 'https://www.reformagkh.ru/myhouse'\n",
    "AREAS_URL = 'https://www.reformagkh.ru/myhouse?tid=2280999&sort=alphabet&item=mkd&ajax'\n",
    "AREAS_FILE = 'regions.json'\n",
    "MUN_URLPAT = 'http://www.reformagkh.ru/myhouse?tid=2280999&sort=alphabet&item=mkd&ajax&tid=%s'\n",
    "MUN_FILEPAT = 'municipal.json'\n",
    "BUILDING_LIST_PAT = 'http://www.reformagkh.ru/myhouse/list?tid=%s&sort=alphabet&item=tp&mkdsort=name&mkdorder=asc&perpage=10000'\n",
    "BUILDING_URLPAT = 'http://www.reformagkh.ru/myhouse/view/%s/?group=0'\n",
    "BUILDING_KEYS = ['house_id', 'url', 'address','year', 'area', 'num_people', 'status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь получим по все АО Москвы и сохраним их в regions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_areas():\n",
    "    \"\"\"Extracts indicators data by area code. Saves as JSON file\"\"\"\n",
    "    filename = AREAS_FILE\n",
    "    if os.path.exists(filename): return None\n",
    "    print(\"Processing Areas\", AREAS_URL)\n",
    "    \n",
    "    url = AREAS_URL\n",
    "    browser = webdriver.PhantomJS(executable_path=\"C:\\\\Users\\\\772\\\\Documents\\\\phantomjs-2.1.1-windows\\\\bin\\\\phantomjs.exe\")\n",
    "    browser.get(url)\n",
    "    sleep(2)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    columns = soup.find_all('div', attrs={'class':'col-4 py-4'})\n",
    "    links = []\n",
    "    for i in columns:\n",
    "        for a in i.find_all('a', href=True):\n",
    "            print(\"Found the URL:\", a['href'])\n",
    "            links.append({\"link\": EXTENDED_BASE + a['href'], \"area_name\": a.text})\n",
    "      \n",
    "    f = open(filename, 'w')\n",
    "    f.write(json.dumps(links, indent=4, ensure_ascii=False))\n",
    "    f.close()\n",
    "    print(\"Processing Areas\", 'written')\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extract_areas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пройдемся по каждому АО для получения списка всех муниципальных округов Москвы и сохраним в municipal.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_municipal():\n",
    "    \"\"\"Extracts data about municipal areas\"\"\"\n",
    "    f = open(AREAS_FILE, 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    js = json.loads(data)\n",
    "    print(js)\n",
    "    links = []\n",
    "\n",
    "    browser = webdriver.Chrome(\"C:\\\\Users\\\\772\\\\Downloads\\\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    #browser = webdriver.Chrome(executable_path=\"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\") \n",
    "    #browser = webdriver.PhantomJS(executable_path=\"C:\\\\Users\\\\772\\\\Docchromedriver.exeuments\\\\phantomjs-2.1.1-windows\\\\bin\\\\phantomjs.exe\")\n",
    "    for area in js: \n",
    "        print(\"обхожу районы\", area['area_name'])\n",
    "        browser.get(area['link'])\n",
    "        sleep(3)\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        columns = soup.find_all('div', attrs={'class':'col-4 py-4'})\n",
    "        for i in columns:\n",
    "            for a in i.find_all('a', href=True):\n",
    "                print(\"Found the URL:\", EXTENDED_BASE + a['href'], a.text)\n",
    "                links.append({\"link\": EXTENDED_BASE + a['href'], \"munuicipal_name\": a.text, \"area_name\": area['area_name'] })\n",
    "                \n",
    "    f = open(MUN_FILEPAT, 'w')\n",
    "    f.write(json.dumps(links, indent=4, ensure_ascii=False))\n",
    "    f.close()\n",
    "    print(\"Processing Areas\", 'written')\n",
    "    return links        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_municipal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для сохранения списка домов со страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_houses(browser, houses):\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    columns = soup.find_all('table', attrs={'class':'table text-center mb-0'})\n",
    "    rows = soup.find_all('tr')\n",
    "    for house in rows: \n",
    "        tds = house.find_all(\"td\")\n",
    "        if len(tds) > 1:\n",
    "            houses.append({\n",
    "                \"adress\": tds[0].text,\n",
    "                \"year\": tds[1].text,\n",
    "                \"square\": tds[2].text,\n",
    "                \"company\": tds[3].text,\n",
    "                \"link\": BASE_URL + tds[0].find('a', href=True)['href']\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь переходим по каждой странице в районе, и с каждой сохраняем все дома в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildings_list(url = None):\n",
    "    if url is None: \n",
    "        f = open(MUN_FILEPAT, 'r')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        js = json.loads(data)\n",
    "        print(js)\n",
    "        url = []\n",
    "        for link in js: \n",
    "            url.append(link['link'])\n",
    "    else:\n",
    "        url = [url]\n",
    "    links = []\n",
    "    browser = webdriver.Chrome(\"C:\\\\Users\\\\772\\\\Downloads\\\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    houses = []\n",
    "\n",
    "    for link in url:\n",
    "        browser.get(link)\n",
    "        while True: \n",
    "            sleep(3)\n",
    "            save_houses(browser, houses)\n",
    "            try:\n",
    "                browser.find_element_by_css_selector('a[aria-label=\"Next\"]').click()\n",
    "            except:\n",
    "                print(\"Все страницы отсмотрены\")\n",
    "                break\n",
    "        f = open(\"lom_houses\", 'w')#тут нужно сохранять с именем района\n",
    "        f.write(json.dumps(houses, indent=4, ensure_ascii=False))\n",
    "        f.close()\n",
    "        print(\"Сохранил\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все страницы отсмотрены\n",
      "Processing Areas written\n"
     ]
    }
   ],
   "source": [
    "get_buildings_list('https://www.reformagkh.ru/myhouse?tid=2281077')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходимся по каждому дому в файле и сохраняем все данные из его паспорта.<br> В закоментированом куске данные по управлению домом. Там довольно грязно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_details(path):\n",
    "    f = open(path, 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    js = json.loads(data)\n",
    "    browser = webdriver.Chrome(\"C:\\\\Users\\\\772\\\\Downloads\\\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    houses_details = []\n",
    "    for house in js:\n",
    "        flag = False\n",
    "        di = {}\n",
    "        di.update({\"ссылка\": house['link'],\n",
    "                  \"адрес\": house[\"adress\"],\n",
    "                  \"УК\": house[\"company\"]})\n",
    "        browser.get(house['link'])\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        rows = soup.find_all('tr')\n",
    "        for tr in rows:\n",
    "            tds = tr.find_all(\"td\")\n",
    "            if flag:\n",
    "                di.update({a[0]: tds[0].text})\n",
    "                di.update({a[1]: tds[1].text})\n",
    "                flag = False\n",
    "                continue\n",
    "            if tds[0].text == \"Тип крыши\":\n",
    "                di.update({tds[0].text : \"\"})\n",
    "                di.update({tds[1].text : \"\"})\n",
    "                a = [tds[0].text,tds[1].text]\n",
    "                flag = True\n",
    "            if len(tds) == 2:\n",
    "                di.update({tds[0].text : tds[1].text})\n",
    "        \n",
    "        houses_details.append(di)\n",
    "        sleep(2)\n",
    "        #browser.find_element_by_partial_link_text(\"Управление\").click()\n",
    "        #sleep(3)\n",
    "        #html = browser.page_source\n",
    "        #soup = BeautifulSoup(html, 'lxml')\n",
    "        #rows = soup.find_all('tr')\n",
    "        #i = 0\n",
    "        #for tr in rows:\n",
    "        #    tds = tr.find_all(\"td\")\n",
    "        #    if len(tds) == 2:\n",
    "        #        di.update({tds[0].text : tds[1].text})\n",
    "        #        i += 1\n",
    "        #        if i == 2:\n",
    "        #            break\n",
    "       \n",
    "    f = open(\"home_small.json\", 'w')#тут нужно сохранять с именем района\n",
    "    f.write(json.dumps(houses_details, indent=4, ensure_ascii=False))\n",
    "    f.close()\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_details(\"small.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json-файл со всеми домами откроем, и пересохраним в xls через pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"home_small.json\", 'r')\n",
    "data = f.read()\n",
    "data = json.loads(data)\n",
    "df = pd.json_normalize(data)\n",
    "df.to_excel(\"lomonosovsky_small.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
